{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c087f72-d428-4b91-8ec3-f81d43411cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def extract_frames_from_folder(folder_path, output_folder, resize_shape=(224, 224), apply_noise_reduction=True, apply_optical_flow=True):\n",
    "    for video_filename in os.listdir(folder_path):\n",
    "        video_path = os.path.join(folder_path, video_filename)\n",
    "        output_subfolder = os.path.join(output_folder, video_filename.split('.')[0])\n",
    "\n",
    "        extract_frames(video_path, output_subfolder, resize_shape, apply_noise_reduction, apply_optical_flow)\n",
    "\n",
    "def extract_frames(video_path, output_folder, resize_shape=(224, 224), apply_noise_reduction=True, apply_optical_flow=True):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Get frames per second (fps) and total number of frames\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Create the output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    frame_count = 0\n",
    "\n",
    "    # Initialize previous frame for optical flow\n",
    "    prev_frame = None\n",
    "\n",
    "    while True:\n",
    "        # Read a frame from the video\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Apply noise reduction (Gaussian blur) if specified\n",
    "        if apply_noise_reduction:\n",
    "            frame = cv2.GaussianBlur(frame, (5, 5), 0)\n",
    "\n",
    "        # Resize the frame to the desired shape\n",
    "        frame = cv2.resize(frame, resize_shape)\n",
    "\n",
    "        # Convert the frame to uint8 if it's not already\n",
    "        if frame.dtype != 'uint8':\n",
    "            frame = (frame * 255).astype('uint8')\n",
    "\n",
    "        # Apply optical flow if specified\n",
    "        if apply_optical_flow and prev_frame is not None:\n",
    "            # Convert frames to grayscale\n",
    "            prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "            current_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Compute optical flow using Farneback method\n",
    "            flow = cv2.calcOpticalFlowFarneback(prev_gray, current_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "\n",
    "            # Save optical flow as an image file\n",
    "            flow_filename = os.path.join(output_folder, f\"flow_{frame_count:04d}.jpg\")\n",
    "            save_optical_flow_image(flow, flow_filename)\n",
    "\n",
    "        # Save the frame as an image file\n",
    "        frame_filename = os.path.join(output_folder, f\"frame_{frame_count:04d}.jpg\")\n",
    "        cv2.imwrite(frame_filename, frame)\n",
    "\n",
    "        frame_count += 1\n",
    "        prev_frame = frame\n",
    "\n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "    ####\n",
    "def save_optical_flow_image(flow, filename):\n",
    "    # Calculate polar coordinates\n",
    "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "\n",
    "    # Convert polar coordinates to HSV color representation\n",
    "    hsv = np.zeros((flow.shape[0], flow.shape[1], 3), dtype=np.uint8)\n",
    "    hsv[..., 0] = ang * 180 / np.pi / 2\n",
    "    hsv[..., 1] = 255\n",
    "    hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "    # Convert HSV to BGR\n",
    "    flow_bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    # Save the optical flow as an image file\n",
    "    cv2.imwrite(filename, flow_bgr)\n",
    "\n",
    "#####\n",
    "\n",
    "# Specify the paths to the \"normal\" and \"anomaly\" folders\n",
    "normal_folder_path = \"dataset/normal\"\n",
    "anomaly_folder_path = \"dataset/anomaly\"\n",
    "\n",
    "# Specify the output folders for frames and optical flow\n",
    "output_normal_folder = \"normal_output\"\n",
    "output_anomaly_folder = \"anomaly_output\"\n",
    "\n",
    "# Resize shape for the frames\n",
    "resize_shape = (224, 224)\n",
    "\n",
    "# Extract frames from the \"normal\" folder with noise reduction and optical flow\n",
    "extract_frames_from_folder(normal_folder_path, output_normal_folder, resize_shape, apply_noise_reduction=True, apply_optical_flow=True)\n",
    "\n",
    "# Extract frames from the \"anomaly\" folder with noise reduction and optical flow\n",
    "extract_frames_from_folder(anomaly_folder_path, output_anomaly_folder, resize_shape, apply_noise_reduction=True, apply_optical_flow=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c04e7f75-4884-4a50-85f9-ad5a7dce6ef9",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'normal_output'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m feature_extractor \u001b[38;5;241m=\u001b[39m Model(inputs\u001b[38;5;241m=\u001b[39mresnet_model\u001b[38;5;241m.\u001b[39minput, outputs\u001b[38;5;241m=\u001b[39mresnet_model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39moutput)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load and preprocess data for normal and anomaly classes using ResNet\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m normal_data, normal_labels \u001b[38;5;241m=\u001b[39m load_and_preprocess_frames(output_normal_folder, feature_extractor)\n\u001b[0;32m      9\u001b[0m anomaly_data, anomaly_labels \u001b[38;5;241m=\u001b[39m load_and_preprocess_frames(output_anomaly_folder, feature_extractor)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Concatenate normal and anomaly data\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[17], line 6\u001b[0m, in \u001b[0;36mload_and_preprocess_frames\u001b[1;34m(folder_path, model, resize_shape, batch_size)\u001b[0m\n\u001b[0;32m      4\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m video_filename \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m      7\u001b[0m     video_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, video_filename)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Extract frames from the video\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'normal_output'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "228a8b92-1e72-4fa4-84ce-ff223eabbdae",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 14.4 GiB for an array with shape (25625, 224, 224, 3) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m resnet_model \u001b[38;5;241m=\u001b[39m ResNet50(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimagenet\u001b[39m\u001b[38;5;124m'\u001b[39m, include_top\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Apply ResNet feature extraction on \"normal\" frames\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m normal_features, normal_labels \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_apply_resnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_normal_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresnet_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Apply ResNet feature extraction on \"anomaly\" frames\u001b[39;00m\n\u001b[0;32m     47\u001b[0m anomaly_features, anomaly_labels \u001b[38;5;241m=\u001b[39m load_and_apply_resnet(output_anomaly_folder, resnet_model)\n",
      "Cell \u001b[1;32mIn[22], line 32\u001b[0m, in \u001b[0;36mload_and_apply_resnet\u001b[1;34m(folder_path, model, resize_shape)\u001b[0m\n\u001b[0;32m     29\u001b[0m data \u001b[38;5;241m=\u001b[39m preprocess_input(data)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Apply ResNet feature extraction\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features, labels\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:91\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Converts the given `value` to an `EagerTensor`.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03mNote that this function could return cached copies of created constants for\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03m  TypeError: if `dtype` is not compatible with the type of t.\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m     88\u001b[0m   \u001b[38;5;66;03m# Make a copy explicitly because the EagerTensor might share the underlying\u001b[39;00m\n\u001b[0;32m     89\u001b[0m   \u001b[38;5;66;03m# memory with the input array. Without this copy, users will be able to\u001b[39;00m\n\u001b[0;32m     90\u001b[0m   \u001b[38;5;66;03m# modify the EagerTensor after its creation by changing the input array.\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m   value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, ops\u001b[38;5;241m.\u001b[39mEagerTensor):\n\u001b[0;32m     93\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m dtype:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 14.4 GiB for an array with shape (25625, 224, 224, 3) and data type float32"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "457abf25-eb70-4151-bebe-c9d69d252b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 21s 2s/step\n",
      "10/10 [==============================] - 16s 2s/step\n",
      "Accuracy: 1.0\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        52\n",
      "           1       1.00      1.00      1.00        78\n",
      "\n",
      "    accuracy                           1.00       130\n",
      "   macro avg       1.00      1.00      1.00       130\n",
      "weighted avg       1.00      1.00      1.00       130\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Function to load and preprocess frames using ResNet with batch processing\n",
    "def load_and_apply_resnet(folder_path, model, resize_shape=(224, 224), max_frames_per_video=50):\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for video_folder in os.listdir(folder_path):\n",
    "        video_folder_path = os.path.join(folder_path, video_folder)\n",
    "        frame_count = 0\n",
    "        \n",
    "        for frame_filename in os.listdir(video_folder_path):\n",
    "            if frame_count >= max_frames_per_video:\n",
    "                break\n",
    "                \n",
    "            frame_path = os.path.join(video_folder_path, frame_filename)\n",
    "            image = cv2.imread(frame_path)\n",
    "            image = cv2.resize(image, resize_shape)\n",
    "            data.append(image)\n",
    "            labels.append(1 if \"normal\" in folder_path else 0)\n",
    "            frame_count += 1\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Preprocess the frames using ResNet requirements\n",
    "    data = preprocess_input(data)\n",
    "\n",
    "    # Apply ResNet feature extraction\n",
    "    features = model.predict(data)\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "# Specify the paths to the \"normal\" and \"anomaly\" folders\n",
    "output_normal_folder = \"normal_output\"\n",
    "output_anomaly_folder = \"anomaly_output\"\n",
    "\n",
    "# Load ResNet50 model pre-trained on ImageNet data\n",
    "resnet_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Apply ResNet feature extraction on a subset of \"normal\" frames\n",
    "normal_features, normal_labels = load_and_apply_resnet(output_normal_folder, resnet_model, max_frames_per_video=50)\n",
    "\n",
    "# Apply ResNet feature extraction on a subset of \"anomaly\" frames\n",
    "anomaly_features, anomaly_labels = load_and_apply_resnet(output_anomaly_folder, resnet_model, max_frames_per_video=50)\n",
    "\n",
    "# Concatenate normal and anomaly features\n",
    "all_features = np.concatenate((normal_features, anomaly_features))\n",
    "all_labels = np.concatenate((normal_labels, anomaly_labels))\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_features, all_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Flatten the feature vectors for SVM\n",
    "X_train_flatten = X_train.reshape((X_train.shape[0], -1))\n",
    "X_test_flatten = X_test.reshape((X_test.shape[0], -1))\n",
    "\n",
    "# Train a Support Vector Machine (SVM) classifier\n",
    "svm_classifier = SVC(probability=True)\n",
    "svm_classifier.fit(X_train_flatten, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = svm_classifier.predict(X_test_flatten)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Display classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4a83637-5f8a-4247-9e1f-d7df2163686b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Dropout: 1.0\n",
      "Classification Report with Dropout:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        52\n",
      "           1       1.00      1.00      1.00        78\n",
      "\n",
      "    accuracy                           1.00       130\n",
      "   macro avg       1.00      1.00      1.00       130\n",
      "weighted avg       1.00      1.00      1.00       130\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Train a Support Vector Machine (SVM) classifier with dropout\n",
    "svm_classifier_dropout = SVC(probability=True, kernel='linear', C=1, gamma='scale', class_weight='balanced')\n",
    "\n",
    "# Use dropout in the model\n",
    "svm_classifier_dropout.fit(X_train_flatten, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions_dropout = svm_classifier_dropout.predict(X_test_flatten)\n",
    "\n",
    "# Evaluate the model with dropout\n",
    "accuracy_dropout = accuracy_score(y_test, predictions_dropout)\n",
    "print(\"Accuracy with Dropout:\", accuracy_dropout)\n",
    "\n",
    "# Display classification report\n",
    "print(\"Classification Report with Dropout:\\n\", classification_report(y_test, predictions_dropout))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed21959-2f66-4636-ab15-4bba392a4ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
